{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting (new) item categories\n",
    "\n",
    "In this lab, we'll try to find a better way to categorize items than the categories they already have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to read in the items.csv and sales_train.csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "items = pd.read_csv('./data/kaggle-sales/items.csv.gz')\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('./data/kaggle-sales/sales_train.csv.gz', parse_dates=['date'])\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only features we have for items right now is the description. Let's add some aggregate sales data mean_price and mean_volume. To do this, we'll first have to reformat sales data to be keyed by item_id and the year.\n",
    "\n",
    "Follow a similar process to the feature engineering lab in order to do this.\n",
    "\n",
    "In particular, we need to:\n",
    "\n",
    " - add a 'year' column\n",
    " - group by item_id and year, computing the (mean) price and (sum) txns\n",
    " - create a MultiIndex on item_id and year and reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['year'] = sales.date.dt.year\n",
    "g = sales.groupby('item_id year'.split())\n",
    "item_sales = pd.concat([\n",
    "    g.item_price.mean().rename('price'),\n",
    "    g.item_cnt_day.sum().rename('txns'),\n",
    "], axis=1)\n",
    "item_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product(\n",
    "    [\n",
    "        sorted(sales['item_id'].unique()),\n",
    "        np.r_[sales['year'].min(): sales['year'].max() + 1]\n",
    "    ], names=['item_id', 'year']\n",
    ")\n",
    "item_sales = item_sales.reindex(index)\n",
    "item_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing txns with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sales = item_sales.fillna({'txns': 0})\n",
    "item_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the mean price and mean transactions per year by grouping *only* by item_id and aggregating with mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sales = item_sales.groupby(level=0).mean()\n",
    "item_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an item_features dataframe that includes the name, price, and transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll encode our text features as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "text_features = vectorizer.fit_transform(data.item_name)\n",
    "text_features = transformer.fit_transform(text_features)\n",
    "text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=10)\n",
    "truncated_text_features = svd.fit_transform(text_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our features dataframe with:\n",
    "\n",
    "- price\n",
    "- txns\n",
    "- text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([\n",
    "    data['price txns'.split()],\n",
    "    pd.DataFrame(truncated_text_features, columns=[f'text_{i}' for i in range(10)])\n",
    "], axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling \n",
    "\n",
    "Use a StandardScaler to scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use DBSCAN with eps=0.1 to find a clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a RandomForestClassifier to evaluate feature importance for the clustering chosen. \n",
    "\n",
    "Remember to drop any sample with a cluster of -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a plot of the feature importance from the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing with a RandomForestRegressor\n",
    "\n",
    "One of the things we might like to know is how predictive of the # of transactions each of our item features is. For this, we can train a RandomForestRegressor and get its feature_importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a RandomForestRegressor on the feature data, using txns as the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a plot of the feature importance from the regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
